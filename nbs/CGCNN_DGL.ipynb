{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "import torch \n",
    "import dgl \n",
    "import torch.nn.functional as F\n",
    "import pickle \n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from dgl.nn.pytorch import Set2Set, NNConv, SetTransformerDecoder, AvgPooling, SumPooling, MaxPooling, SortPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrystalLoader(object):\n",
    "    def __init__(self,path_to_pkl):\n",
    "        self.path_to_pkl = path_to_pkl\n",
    "        self._load()\n",
    "    def _load(self):\n",
    "        with open(self.path_to_pkl,'rb') as infile:\n",
    "            self.graphs = pickle.load(infile)\n",
    "        self.targets = torch.Tensor(np.array([graph.target for graph in self.graphs]).reshape(-1, 1).astype(np.float32))\n",
    "        self.details =[{'adsorbate':graph.adsorbate,\n",
    "         'miller':graph.miller,\n",
    "         'comp':graph.comp,\n",
    "          'mpid':graph.mpid} for graph in self.graphs]\n",
    "    def __getitem__(self, item):\n",
    "        g, target = self.graphs[item], self.targets[item]\n",
    "        return g, target\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CrystalLoader('/scratch/westgroup/mpnn/gasdb_dgl_graphs/init_gasdb_dgl_graphs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_crystal_graphs_for_regression(data):\n",
    "    graphs, targets = map(list, zip(*data))\n",
    "    bg = dgl.batch(graphs)\n",
    "    bg.set_n_initializer(dgl.init.zero_initializer)\n",
    "    bg.set_e_initializer(dgl.init.zero_initializer)\n",
    "    labels = torch.stack(targets, dim=0)\n",
    "    return bg, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        data,\n",
    "        batch_size=20,\n",
    "        collate_fn=collate_crystal_graphs_for_regression,\n",
    "        num_workers=8,)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "        data[800:],\n",
    "        batch_size=20,\n",
    "        collate_fn=collate_crystal_graphs_for_regression,\n",
    "        num_workers=8,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CG(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                atom_hidden_feats,\n",
    "                bond_in_feats,\n",
    "                 ):\n",
    "        super(CG, self).__init__()\n",
    "        self.atom_hidden_feats = atom_hidden_feats\n",
    "        self.bond_in_feats = bond_in_feats\n",
    "        self.lin = torch.nn.Linear(2*atom_hidden_feats+bond_in_feats,\n",
    "                                   atom_hidden_feats,bias=True)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.softplus = torch.nn.LeakyReLU()\n",
    "\n",
    "    def get_msg(self, edges):\n",
    "        z = torch.cat([edges.src['v'], edges.dst['v'],edges.data['gdf_feat']], -1)\n",
    "        z = self.lin(z)\n",
    "        sig_z = self.sigmoid(z)\n",
    "        softplus_z = self.softplus(z)\n",
    "        return {'z':sig_z*softplus_z}\n",
    "    def forward(self, graph, feat):\n",
    "        graph = graph.local_var()\n",
    "        graph.ndata['v'] = feat\n",
    "        graph.update_all(message_func=self.get_msg,\n",
    "                     reduce_func=fn.sum('z', 'm'))\n",
    "        return graph.ndata['m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGConvNet(torch.nn.Module):\n",
    "    def __init__(self, atom_in_feats=100, \n",
    "                 atom_hidden_feats=64,\n",
    "                 bond_in_feats=41,\n",
    "                 n_conv=3,\n",
    "                 n_h=1,\n",
    "                 graph_rep_dim=64,\n",
    "                 pooling='sum'\n",
    "                ):\n",
    "        super(CGConvNet, self).__init__()\n",
    "        self.atom_in_feats = atom_in_feats\n",
    "        self.atom_hidden_feats = atom_hidden_feats\n",
    "        self.bond_in_feats = bond_in_feats\n",
    "        self.n_conv = n_conv\n",
    "        self.n_h = n_h\n",
    "        self.graph_rep_dim = graph_rep_dim\n",
    "        self.conv_layer = CG(atom_hidden_feats=atom_hidden_feats,\n",
    "                             bond_in_feats = bond_in_feats\n",
    "                            )\n",
    "        if pooling=='sum':\n",
    "            self.pooling_module = SumPooling()\n",
    "        self.lin0 = torch.nn.Linear(atom_in_feats, atom_hidden_feats, bias=True)\n",
    "        self.lin1 = torch.nn.Linear(atom_hidden_feats,graph_rep_dim,bias=True)\n",
    "        self.lin2 = torch.nn.Linear(graph_rep_dim,1,bias=True)\n",
    "        \n",
    "    def forward(self,graph):\n",
    "        n_feat = graph.ndata.pop('n_feat')   #B, N, atom_in_feats\n",
    "        out = F.relu(self.lin0(n_feat))\n",
    "        #v = out.unsqueeze(0) \n",
    "        for i in range(self.n_conv):\n",
    "            #print(f\"running iteration {i}\")\n",
    "            out = self.conv_layer(graph, out)\n",
    "        graph_rep = F.relu(self.pooling_module(graph,out))\n",
    "        preds = F.relu(self.lin2(graph_rep))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, evaluation):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    error_ratio = AverageMeter()\n",
    "    total_loss = 0\n",
    "    total_error = 0\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for batch_id, batch_data in enumerate(train_loader):\n",
    "        # Prepare input data\n",
    "        if torch.cuda.is_available():\n",
    "            bg, labels = batch_data\n",
    "            bg.to(torch.device('cuda'))\n",
    "            labels = labels.to('cuda')\n",
    "       \n",
    "        # Measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute output\n",
    "        output = model(bg)\n",
    "        train_loss = criterion(output, labels)\n",
    "        train_eval = evaluation(output, labels)\n",
    "        # Logs\n",
    "        losses.update(criterion(output, labels).item(), bg.batch_size)\n",
    "        error_ratio.update((evaluation(output, labels)).item(), bg.batch_size)\n",
    "        total_loss += train_loss*bg.batch_size\n",
    "        total_error += train_eval*bg.batch_size\n",
    "        # compute gradient and do SGD step\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if batch_id % 20 == 0 and batch_id > 0:\n",
    "\n",
    "            print(\n",
    "                'Epoch: [{0}][{1}/{2}]\\t'\n",
    "                'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                'Error Ratio {err.val:.4f} ({err.avg:.4f}) \\t' .format(\n",
    "                    epoch,\n",
    "                    batch_id,\n",
    "                    len(train_loader),\n",
    "                    batch_time=batch_time,\n",
    "                    data_time=data_time,\n",
    "                    loss=losses,\n",
    "                    err=error_ratio))\n",
    "    total_loss = total_loss /len(train_loader.dataset)\n",
    "    total_error = total_error / len(train_loader.dataset)\n",
    "    print('epoch {:d}/{:d}, training loss {:.4f}, training score {:.4f}'.format(\n",
    "        epoch + 1, 100, total_loss, total_error))\n",
    "    print(os.system(\"nvidia-smi\"))\n",
    "\n",
    "    print('Epoch: [{0}] Avg Error Ratio {err.avg:.3f}; Average Loss {loss.avg:.3f}; Avg Time x Batch {b_time.avg:.3f}'\n",
    "          .format(epoch, err=error_ratio, loss=losses, b_time=batch_time))\n",
    "    return total_loss.detach().cpu().numpy(), total_error.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CGConvNet()\n",
    "criterion = torch.nn.MSELoss()\n",
    "evaluation = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model =model.to('cuda')\n",
    "    criterion = criterion.to('cuda')\n",
    "    evaluation = evaluation.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][20/50]\tTime 0.019 (0.062)\tData 0.014 (0.039)\tLoss 367950.9375 (619385.5610)\tError Ratio 483.4173 (656.7005) \t\n",
      "Epoch: [0][40/50]\tTime 0.012 (0.040)\tData 0.007 (0.026)\tLoss 171229.1562 (447378.5574)\tError Ratio 336.8304 (545.2579) \t\n",
      "epoch 1/100, training loss 395418.6562, training score 504.5711\n",
      "0\n",
      "Epoch: [0] Avg Error Ratio 504.571; Average Loss 395418.562; Avg Time x Batch 0.035\n",
      "Epoch: [1][20/50]\tTime 0.020 (0.045)\tData 0.014 (0.038)\tLoss 29471.1309 (86110.2975)\tError Ratio 115.5363 (199.7495) \t\n",
      "Epoch: [1][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 14535.5625 (63604.4175)\tError Ratio 73.9134 (163.2737) \t\n",
      "epoch 2/100, training loss 57438.2109, training score 152.4424\n",
      "0\n",
      "Epoch: [1] Avg Error Ratio 152.442; Average Loss 57438.206; Avg Time x Batch 0.028\n",
      "Epoch: [2][20/50]\tTime 0.020 (0.047)\tData 0.014 (0.039)\tLoss 4310.2603 (22092.5839)\tError Ratio 33.9297 (73.2555) \t\n",
      "Epoch: [2][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 2237.0410 (16776.6507)\tError Ratio 24.2140 (61.3715) \t\n",
      "epoch 3/100, training loss 15541.5098, training score 58.1424\n",
      "0\n",
      "Epoch: [2] Avg Error Ratio 58.142; Average Loss 15541.510; Avg Time x Batch 0.028\n",
      "Epoch: [3][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.037)\tLoss 1246.1082 (9528.0564)\tError Ratio 16.5498 (37.0614) \t\n",
      "Epoch: [3][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 565.0841 (7329.7836)\tError Ratio 7.9887 (30.6973) \t\n",
      "epoch 4/100, training loss 6887.7476, training score 29.5123\n",
      "0\n",
      "Epoch: [3] Avg Error Ratio 29.512; Average Loss 6887.748; Avg Time x Batch 0.028\n",
      "Epoch: [4][20/50]\tTime 0.020 (0.047)\tData 0.014 (0.038)\tLoss 342.7182 (5397.5280)\tError Ratio 8.5953 (22.8692) \t\n",
      "Epoch: [4][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 242.6486 (4204.7329)\tError Ratio 5.0757 (19.0490) \t\n",
      "epoch 5/100, training loss 3973.3337, training score 18.4677\n",
      "0\n",
      "Epoch: [4] Avg Error Ratio 18.468; Average Loss 3973.334; Avg Time x Batch 0.028\n",
      "Epoch: [5][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.037)\tLoss 78.6050 (3461.0457)\tError Ratio 3.1006 (16.1032) \t\n",
      "Epoch: [5][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 123.2598 (2714.2528)\tError Ratio 3.0068 (13.3405) \t\n",
      "epoch 6/100, training loss 2565.2183, training score 13.0353\n",
      "0\n",
      "Epoch: [5] Avg Error Ratio 13.035; Average Loss 2565.218; Avg Time x Batch 0.028\n",
      "Epoch: [6][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.037)\tLoss 32.4955 (2347.7077)\tError Ratio 1.9919 (12.0162) \t\n",
      "Epoch: [6][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 71.3201 (1856.9836)\tError Ratio 2.4100 (9.9851) \t\n",
      "epoch 7/100, training loss 1753.5907, training score 9.7596\n",
      "0\n",
      "Epoch: [6] Avg Error Ratio 9.760; Average Loss 1753.591; Avg Time x Batch 0.028\n",
      "Epoch: [7][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.037)\tLoss 11.1105 (1663.4676)\tError Ratio 1.4464 (9.2270) \t\n",
      "Epoch: [7][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 39.4510 (1329.5339)\tError Ratio 1.9222 (7.7246) \t\n",
      "epoch 8/100, training loss 1254.9447, training score 7.5778\n",
      "0\n",
      "Epoch: [7] Avg Error Ratio 7.578; Average Loss 1254.945; Avg Time x Batch 0.028\n",
      "Epoch: [8][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.037)\tLoss 2.4309 (1217.9827)\tError Ratio 0.9967 (7.4083) \t\n",
      "Epoch: [8][40/50]\tTime 0.012 (0.032)\tData 0.007 (0.025)\tLoss 20.1215 (987.1429)\tError Ratio 1.5146 (6.2251) \t\n",
      "epoch 9/100, training loss 932.0946, training score 6.0872\n",
      "0\n",
      "Epoch: [8] Avg Error Ratio 6.087; Average Loss 932.095; Avg Time x Batch 0.028\n",
      "Epoch: [9][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.038)\tLoss 1.3807 (915.9280)\tError Ratio 0.8569 (6.0261) \t\n",
      "Epoch: [9][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 9.0940 (756.1844)\tError Ratio 1.1752 (5.1492) \t\n",
      "epoch 10/100, training loss 715.7527, training score 5.0280\n",
      "0\n",
      "Epoch: [9] Avg Error Ratio 5.028; Average Loss 715.753; Avg Time x Batch 0.028\n",
      "Epoch: [10][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.038)\tLoss 1.6279 (706.3425)\tError Ratio 0.9011 (5.0386) \t\n",
      "Epoch: [10][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 3.3860 (596.1093)\tError Ratio 0.8897 (4.3552) \t\n",
      "epoch 11/100, training loss 565.4962, training score 4.2738\n",
      "0\n",
      "Epoch: [10] Avg Error Ratio 4.274; Average Loss 565.496; Avg Time x Batch 0.028\n",
      "Epoch: [11][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.038)\tLoss 1.6279 (554.2658)\tError Ratio 0.9011 (4.3085) \t\n",
      "Epoch: [11][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 1.0702 (480.3112)\tError Ratio 0.6407 (3.7556) \t\n",
      "epoch 12/100, training loss 456.0323, training score 3.6954\n",
      "0\n",
      "Epoch: [11] Avg Error Ratio 3.695; Average Loss 456.032; Avg Time x Batch 0.028\n",
      "Epoch: [12][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.038)\tLoss 1.6279 (439.5205)\tError Ratio 0.9011 (3.7880) \t\n",
      "Epoch: [12][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 1.0940 (393.4469)\tError Ratio 0.6461 (3.3448) \t\n",
      "epoch 13/100, training loss 373.5632, training score 3.2852\n",
      "0\n",
      "Epoch: [12] Avg Error Ratio 3.285; Average Loss 373.563; Avg Time x Batch 0.028\n",
      "Epoch: [13][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.038)\tLoss 1.6279 (349.3207)\tError Ratio 0.9011 (3.3689) \t\n",
      "Epoch: [13][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (324.6287)\tError Ratio 0.6514 (3.0266) \t\n",
      "epoch 14/100, training loss 308.2640, training score 2.9668\n",
      "0\n",
      "Epoch: [13] Avg Error Ratio 2.967; Average Loss 308.264; Avg Time x Batch 0.028\n",
      "Epoch: [14][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.038)\tLoss 1.6279 (277.5742)\tError Ratio 0.9011 (2.9999) \t\n",
      "Epoch: [14][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (269.4895)\tError Ratio 0.6514 (2.7434) \t\n",
      "epoch 15/100, training loss 256.0606, training score 2.6820\n",
      "0\n",
      "Epoch: [14] Avg Error Ratio 2.682; Average Loss 256.061; Avg Time x Batch 0.028\n",
      "Epoch: [15][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.037)\tLoss 1.6279 (221.1575)\tError Ratio 0.9011 (2.6806) \t\n",
      "Epoch: [15][40/50]\tTime 0.011 (0.034)\tData 0.006 (0.025)\tLoss 1.1189 (225.7371)\tError Ratio 0.6514 (2.5068) \t\n",
      "epoch 16/100, training loss 214.7561, training score 2.4402\n",
      "0\n",
      "Epoch: [15] Avg Error Ratio 2.440; Average Loss 214.756; Avg Time x Batch 0.030\n",
      "Epoch: [16][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.038)\tLoss 1.6279 (177.1474)\tError Ratio 0.9011 (2.3901) \t\n",
      "Epoch: [16][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (190.6850)\tError Ratio 0.6514 (2.3066) \t\n",
      "epoch 17/100, training loss 181.8264, training score 2.2405\n",
      "0\n",
      "Epoch: [16] Avg Error Ratio 2.240; Average Loss 181.826; Avg Time x Batch 0.028\n",
      "Epoch: [17][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.037)\tLoss 1.6279 (142.9099)\tError Ratio 0.9011 (2.1527) \t\n",
      "Epoch: [17][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (162.2989)\tError Ratio 0.6514 (2.1376) \t\n",
      "epoch 18/100, training loss 155.0842, training score 2.0805\n",
      "0\n",
      "Epoch: [17] Avg Error Ratio 2.080; Average Loss 155.084; Avg Time x Batch 0.028\n",
      "Epoch: [18][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.038)\tLoss 1.6279 (116.4006)\tError Ratio 0.9011 (1.9272) \t\n",
      "Epoch: [18][40/50]\tTime 0.012 (0.032)\tData 0.007 (0.025)\tLoss 1.1189 (139.2604)\tError Ratio 0.6514 (1.9782) \t\n",
      "epoch 19/100, training loss 133.2413, training score 1.9299\n",
      "0\n",
      "Epoch: [18] Avg Error Ratio 1.930; Average Loss 133.241; Avg Time x Batch 0.028\n",
      "Epoch: [19][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.038)\tLoss 1.6279 (95.7971)\tError Ratio 0.9011 (1.7722) \t\n",
      "Epoch: [19][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (120.3421)\tError Ratio 0.6514 (1.8576) \t\n",
      "epoch 20/100, training loss 115.1856, training score 1.8124\n",
      "0\n",
      "Epoch: [19] Avg Error Ratio 1.812; Average Loss 115.186; Avg Time x Batch 0.028\n",
      "Epoch: [20][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.037)\tLoss 1.6279 (79.0091)\tError Ratio 0.9011 (1.6405) \t\n",
      "Epoch: [20][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (104.3049)\tError Ratio 0.6514 (1.7512) \t\n",
      "epoch 21/100, training loss 99.8381, training score 1.7091\n",
      "0\n",
      "Epoch: [20] Avg Error Ratio 1.709; Average Loss 99.838; Avg Time x Batch 0.028\n",
      "Epoch: [21][20/50]\tTime 0.021 (0.046)\tData 0.015 (0.037)\tLoss 1.6279 (65.3928)\tError Ratio 0.9011 (1.5174) \t\n",
      "Epoch: [21][40/50]\tTime 0.012 (0.032)\tData 0.007 (0.025)\tLoss 1.1189 (90.7442)\tError Ratio 0.6514 (1.6521) \t\n",
      "epoch 22/100, training loss 86.8238, training score 1.6135\n",
      "0\n",
      "Epoch: [21] Avg Error Ratio 1.613; Average Loss 86.824; Avg Time x Batch 0.028\n",
      "Epoch: [22][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.038)\tLoss 1.6279 (54.4953)\tError Ratio 0.9011 (1.4161) \t\n",
      "Epoch: [22][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (79.3515)\tError Ratio 0.6514 (1.5700) \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23/100, training loss 75.8534, training score 1.5327\n",
      "0\n",
      "Epoch: [22] Avg Error Ratio 1.533; Average Loss 75.853; Avg Time x Batch 0.028\n",
      "Epoch: [23][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.038)\tLoss 1.6279 (45.7995)\tError Ratio 0.9011 (1.3419) \t\n",
      "Epoch: [23][40/50]\tTime 0.012 (0.032)\tData 0.007 (0.025)\tLoss 1.1189 (69.7498)\tError Ratio 0.6514 (1.5027) \t\n",
      "epoch 24/100, training loss 66.5711, training score 1.4672\n",
      "0\n",
      "Epoch: [23] Avg Error Ratio 1.467; Average Loss 66.571; Avg Time x Batch 0.028\n",
      "Epoch: [24][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.038)\tLoss 1.6279 (38.5685)\tError Ratio 0.9011 (1.2712) \t\n",
      "Epoch: [24][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (61.4444)\tError Ratio 0.6514 (1.4376) \t\n",
      "epoch 25/100, training loss 58.5187, training score 1.4055\n",
      "0\n",
      "Epoch: [24] Avg Error Ratio 1.405; Average Loss 58.519; Avg Time x Batch 0.028\n",
      "Epoch: [25][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.037)\tLoss 1.6279 (32.5850)\tError Ratio 0.9011 (1.2041) \t\n",
      "Epoch: [25][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (54.2739)\tError Ratio 0.6514 (1.3759) \t\n",
      "epoch 26/100, training loss 51.5458, training score 1.3469\n",
      "0\n",
      "Epoch: [25] Avg Error Ratio 1.347; Average Loss 51.546; Avg Time x Batch 0.028\n",
      "Epoch: [26][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.038)\tLoss 1.6279 (27.7113)\tError Ratio 0.9011 (1.1408) \t\n",
      "Epoch: [26][40/50]\tTime 0.012 (0.032)\tData 0.007 (0.025)\tLoss 1.1189 (48.1321)\tError Ratio 0.6514 (1.3176) \t\n",
      "epoch 27/100, training loss 45.5526, training score 1.2917\n",
      "0\n",
      "Epoch: [26] Avg Error Ratio 1.292; Average Loss 45.553; Avg Time x Batch 0.028\n",
      "Epoch: [27][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.038)\tLoss 1.6279 (23.7119)\tError Ratio 0.9011 (1.0950) \t\n",
      "Epoch: [27][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (42.8070)\tError Ratio 0.6514 (1.2694) \t\n",
      "epoch 28/100, training loss 40.3392, training score 1.2450\n",
      "0\n",
      "Epoch: [27] Avg Error Ratio 1.245; Average Loss 40.339; Avg Time x Batch 0.028\n",
      "Epoch: [28][20/50]\tTime 0.020 (0.046)\tData 0.014 (0.038)\tLoss 1.6279 (20.2515)\tError Ratio 0.9011 (1.0556) \t\n",
      "Epoch: [28][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (38.0854)\tError Ratio 0.6514 (1.2254) \t\n",
      "epoch 29/100, training loss 35.7185, training score 1.2022\n",
      "0\n",
      "Epoch: [28] Avg Error Ratio 1.202; Average Loss 35.719; Avg Time x Batch 0.028\n",
      "Epoch: [29][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.038)\tLoss 1.6279 (17.2663)\tError Ratio 0.9011 (1.0178) \t\n",
      "Epoch: [29][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (33.9105)\tError Ratio 0.6514 (1.1834) \t\n",
      "epoch 30/100, training loss 31.6369, training score 1.1612\n",
      "0\n",
      "Epoch: [29] Avg Error Ratio 1.161; Average Loss 31.637; Avg Time x Batch 0.028\n",
      "Epoch: [30][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.038)\tLoss 1.6279 (14.7156)\tError Ratio 0.9011 (0.9818) \t\n",
      "Epoch: [30][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (30.2409)\tError Ratio 0.6514 (1.1432) \t\n",
      "epoch 31/100, training loss 28.0525, training score 1.1220\n",
      "0\n",
      "Epoch: [30] Avg Error Ratio 1.122; Average Loss 28.053; Avg Time x Batch 0.028\n",
      "Epoch: [31][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.037)\tLoss 1.6279 (12.5589)\tError Ratio 0.9011 (0.9474) \t\n",
      "Epoch: [31][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (27.0356)\tError Ratio 0.6514 (1.1050) \t\n",
      "epoch 32/100, training loss 24.9249, training score 1.0849\n",
      "0\n",
      "Epoch: [31] Avg Error Ratio 1.085; Average Loss 24.925; Avg Time x Batch 0.028\n",
      "Epoch: [32][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.038)\tLoss 1.6279 (10.7577)\tError Ratio 0.9011 (0.9149) \t\n",
      "Epoch: [32][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (24.2553)\tError Ratio 0.6514 (1.0688) \t\n",
      "epoch 33/100, training loss 22.2144, training score 1.0497\n",
      "0\n",
      "Epoch: [32] Avg Error Ratio 1.050; Average Loss 22.214; Avg Time x Batch 0.028\n",
      "Epoch: [33][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.038)\tLoss 1.6279 (9.2717)\tError Ratio 0.9011 (0.8841) \t\n",
      "Epoch: [33][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (21.8560)\tError Ratio 0.6514 (1.0346) \t\n",
      "epoch 34/100, training loss 19.8778, training score 1.0164\n",
      "0\n",
      "Epoch: [33] Avg Error Ratio 1.016; Average Loss 19.878; Avg Time x Batch 0.028\n",
      "Epoch: [34][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (8.0412)\tError Ratio 0.9011 (0.8630) \t\n",
      "Epoch: [34][40/50]\tTime 0.013 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (19.7745)\tError Ratio 0.6514 (1.0063) \t\n",
      "epoch 35/100, training loss 17.8531, training score 0.9883\n",
      "0\n",
      "Epoch: [34] Avg Error Ratio 0.988; Average Loss 17.853; Avg Time x Batch 0.029\n",
      "Epoch: [35][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (6.9902)\tError Ratio 0.9011 (0.8436) \t\n",
      "Epoch: [35][40/50]\tTime 0.013 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (17.9481)\tError Ratio 0.6514 (0.9814) \t\n",
      "epoch 36/100, training loss 16.0820, training score 0.9632\n",
      "0\n",
      "Epoch: [35] Avg Error Ratio 0.963; Average Loss 16.082; Avg Time x Batch 0.029\n",
      "Epoch: [36][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (6.0953)\tError Ratio 0.9011 (0.8252) \t\n",
      "Epoch: [36][40/50]\tTime 0.012 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (16.3255)\tError Ratio 0.6514 (0.9600) \t\n",
      "epoch 37/100, training loss 14.5158, training score 0.9412\n",
      "0\n",
      "Epoch: [36] Avg Error Ratio 0.941; Average Loss 14.516; Avg Time x Batch 0.029\n",
      "Epoch: [37][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.038)\tLoss 1.6279 (5.3348)\tError Ratio 0.9011 (0.8076) \t\n",
      "Epoch: [37][40/50]\tTime 0.012 (0.032)\tData 0.007 (0.025)\tLoss 1.1189 (14.8733)\tError Ratio 0.6514 (0.9396) \t\n",
      "epoch 38/100, training loss 13.1236, training score 0.9202\n",
      "0\n",
      "Epoch: [37] Avg Error Ratio 0.920; Average Loss 13.124; Avg Time x Batch 0.028\n",
      "Epoch: [38][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (4.6960)\tError Ratio 0.9011 (0.7908) \t\n",
      "Epoch: [38][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (13.5795)\tError Ratio 0.6514 (0.9201) \t\n",
      "epoch 39/100, training loss 11.8922, training score 0.9001\n",
      "0\n",
      "Epoch: [38] Avg Error Ratio 0.900; Average Loss 11.892; Avg Time x Batch 0.029\n",
      "Epoch: [39][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (4.1664)\tError Ratio 0.9011 (0.7748) \t\n",
      "Epoch: [39][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (12.4306)\tError Ratio 0.6514 (0.9016) \t\n",
      "epoch 40/100, training loss 10.8078, training score 0.8810\n",
      "0\n",
      "Epoch: [39] Avg Error Ratio 0.881; Average Loss 10.808; Avg Time x Batch 0.029\n",
      "Epoch: [40][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (3.7328)\tError Ratio 0.9011 (0.7597) \t\n",
      "Epoch: [40][40/50]\tTime 0.012 (0.032)\tData 0.007 (0.025)\tLoss 1.1189 (11.4128)\tError Ratio 0.6514 (0.8840) \t\n",
      "epoch 41/100, training loss 9.8561, training score 0.8629\n",
      "0\n",
      "Epoch: [40] Avg Error Ratio 0.863; Average Loss 9.856; Avg Time x Batch 0.028\n",
      "Epoch: [41][20/50]\tTime 0.021 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (3.3837)\tError Ratio 0.9011 (0.7454) \t\n",
      "Epoch: [41][40/50]\tTime 0.012 (0.032)\tData 0.007 (0.025)\tLoss 1.1189 (10.5142)\tError Ratio 0.6514 (0.8673) \t\n",
      "epoch 42/100, training loss 9.0244, training score 0.8458\n",
      "0\n",
      "Epoch: [41] Avg Error Ratio 0.846; Average Loss 9.024; Avg Time x Batch 0.028\n",
      "Epoch: [42][20/50]\tTime 0.021 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (3.1079)\tError Ratio 0.9011 (0.7318) \t\n",
      "Epoch: [42][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.025)\tLoss 1.1189 (9.7222)\tError Ratio 0.6514 (0.8520) \t\n",
      "epoch 43/100, training loss 8.3000, training score 0.8299\n",
      "0\n",
      "Epoch: [42] Avg Error Ratio 0.830; Average Loss 8.300; Avg Time x Batch 0.029\n",
      "Epoch: [43][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (2.8940)\tError Ratio 0.9011 (0.7217) \t\n",
      "Epoch: [43][40/50]\tTime 0.012 (0.032)\tData 0.007 (0.025)\tLoss 1.1189 (9.0223)\tError Ratio 0.6514 (0.8393) \t\n",
      "epoch 44/100, training loss 7.6682, training score 0.8163\n",
      "0\n",
      "Epoch: [43] Avg Error Ratio 0.816; Average Loss 7.668; Avg Time x Batch 0.028\n",
      "Epoch: [44][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (2.7210)\tError Ratio 0.9011 (0.7187) \t\n",
      "Epoch: [44][40/50]\tTime 0.013 (0.032)\tData 0.008 (0.025)\tLoss 1.1189 (8.3922)\tError Ratio 0.6514 (0.8305) \t\n",
      "epoch 45/100, training loss 7.1083, training score 0.8061\n",
      "0\n",
      "Epoch: [44] Avg Error Ratio 0.806; Average Loss 7.108; Avg Time x Batch 0.029\n",
      "Epoch: [45][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (2.5612)\tError Ratio 0.9011 (0.7157) \t\n",
      "Epoch: [45][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (7.8104)\tError Ratio 0.6514 (0.8220) \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46/100, training loss 6.6008, training score 0.7963\n",
      "0\n",
      "Epoch: [45] Avg Error Ratio 0.796; Average Loss 6.601; Avg Time x Batch 0.028\n",
      "Epoch: [46][20/50]\tTime 0.021 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (2.4133)\tError Ratio 0.9011 (0.7128) \t\n",
      "Epoch: [46][40/50]\tTime 0.013 (0.033)\tData 0.007 (0.025)\tLoss 1.1189 (7.2730)\tError Ratio 0.6514 (0.8138) \t\n",
      "epoch 47/100, training loss 6.1412, training score 0.7868\n",
      "0\n",
      "Epoch: [46] Avg Error Ratio 0.787; Average Loss 6.141; Avg Time x Batch 0.029\n",
      "Epoch: [47][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (2.2767)\tError Ratio 0.9011 (0.7101) \t\n",
      "Epoch: [47][40/50]\tTime 0.012 (0.032)\tData 0.007 (0.025)\tLoss 1.1189 (6.7787)\tError Ratio 0.6514 (0.8059) \t\n",
      "epoch 48/100, training loss 5.7298, training score 0.7786\n",
      "0\n",
      "Epoch: [47] Avg Error Ratio 0.779; Average Loss 5.730; Avg Time x Batch 0.029\n",
      "Epoch: [48][20/50]\tTime 0.021 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (2.1515)\tError Ratio 0.9011 (0.7074) \t\n",
      "Epoch: [48][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.025)\tLoss 1.1189 (6.3281)\tError Ratio 0.6514 (0.7983) \t\n",
      "epoch 49/100, training loss 5.3602, training score 0.7721\n",
      "0\n",
      "Epoch: [48] Avg Error Ratio 0.772; Average Loss 5.360; Avg Time x Batch 0.029\n",
      "Epoch: [49][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.037)\tLoss 1.6279 (2.0347)\tError Ratio 0.9011 (0.7048) \t\n",
      "Epoch: [49][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (5.9055)\tError Ratio 0.6514 (0.7909) \t\n",
      "epoch 50/100, training loss 5.0136, training score 0.7660\n",
      "0\n",
      "Epoch: [49] Avg Error Ratio 0.766; Average Loss 5.014; Avg Time x Batch 0.028\n",
      "Epoch: [50][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (1.9259)\tError Ratio 0.9011 (0.7022) \t\n",
      "Epoch: [50][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.025)\tLoss 1.1189 (5.5095)\tError Ratio 0.6514 (0.7837) \t\n",
      "epoch 51/100, training loss 4.6889, training score 0.7601\n",
      "0\n",
      "Epoch: [50] Avg Error Ratio 0.760; Average Loss 4.689; Avg Time x Batch 0.029\n",
      "Epoch: [51][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (1.8248)\tError Ratio 0.9011 (0.6997) \t\n",
      "Epoch: [51][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (5.1393)\tError Ratio 0.6514 (0.7766) \t\n",
      "epoch 52/100, training loss 4.3854, training score 0.7542\n",
      "0\n",
      "Epoch: [51] Avg Error Ratio 0.754; Average Loss 4.385; Avg Time x Batch 0.029\n",
      "Epoch: [52][20/50]\tTime 0.022 (0.047)\tData 0.016 (0.039)\tLoss 1.6279 (1.7312)\tError Ratio 0.9011 (0.6973) \t\n",
      "Epoch: [52][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.026)\tLoss 1.1189 (4.7941)\tError Ratio 0.6514 (0.7696) \t\n",
      "epoch 53/100, training loss 4.1023, training score 0.7485\n",
      "0\n",
      "Epoch: [52] Avg Error Ratio 0.749; Average Loss 4.102; Avg Time x Batch 0.029\n",
      "Epoch: [53][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (1.6449)\tError Ratio 0.9011 (0.6949) \t\n",
      "Epoch: [53][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.026)\tLoss 1.1189 (4.4730)\tError Ratio 0.6514 (0.7629) \t\n",
      "epoch 54/100, training loss 3.8390, training score 0.7430\n",
      "0\n",
      "Epoch: [53] Avg Error Ratio 0.743; Average Loss 3.839; Avg Time x Batch 0.029\n",
      "Epoch: [54][20/50]\tTime 0.021 (0.047)\tData 0.016 (0.039)\tLoss 1.6279 (1.5653)\tError Ratio 0.9011 (0.6926) \t\n",
      "Epoch: [54][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.026)\tLoss 1.1189 (4.1749)\tError Ratio 0.6514 (0.7563) \t\n",
      "epoch 55/100, training loss 3.5945, training score 0.7376\n",
      "0\n",
      "Epoch: [54] Avg Error Ratio 0.738; Average Loss 3.595; Avg Time x Batch 0.029\n",
      "Epoch: [55][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (1.4924)\tError Ratio 0.9011 (0.6904) \t\n",
      "Epoch: [55][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.025)\tLoss 1.1189 (3.8988)\tError Ratio 0.6514 (0.7499) \t\n",
      "epoch 56/100, training loss 3.3682, training score 0.7323\n",
      "0\n",
      "Epoch: [55] Avg Error Ratio 0.732; Average Loss 3.368; Avg Time x Batch 0.029\n",
      "Epoch: [56][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (1.4257)\tError Ratio 0.9011 (0.6882) \t\n",
      "Epoch: [56][40/50]\tTime 0.012 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (3.6439)\tError Ratio 0.6514 (0.7437) \t\n",
      "epoch 57/100, training loss 3.1591, training score 0.7273\n",
      "0\n",
      "Epoch: [56] Avg Error Ratio 0.727; Average Loss 3.159; Avg Time x Batch 0.029\n",
      "Epoch: [57][20/50]\tTime 0.020 (0.046)\tData 0.015 (0.038)\tLoss 1.6279 (1.3650)\tError Ratio 0.9011 (0.6861) \t\n",
      "Epoch: [57][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (3.4089)\tError Ratio 0.6514 (0.7376) \t\n",
      "epoch 58/100, training loss 2.9664, training score 0.7223\n",
      "0\n",
      "Epoch: [57] Avg Error Ratio 0.722; Average Loss 2.966; Avg Time x Batch 0.028\n",
      "Epoch: [58][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (1.3098)\tError Ratio 0.9011 (0.6841) \t\n",
      "Epoch: [58][40/50]\tTime 0.013 (0.033)\tData 0.008 (0.026)\tLoss 1.1189 (3.1928)\tError Ratio 0.6514 (0.7318) \t\n",
      "epoch 59/100, training loss 2.7892, training score 0.7175\n",
      "0\n",
      "Epoch: [58] Avg Error Ratio 0.718; Average Loss 2.789; Avg Time x Batch 0.029\n",
      "Epoch: [59][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (1.2599)\tError Ratio 0.9011 (0.6821) \t\n",
      "Epoch: [59][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (2.9946)\tError Ratio 0.6514 (0.7262) \t\n",
      "epoch 60/100, training loss 2.6267, training score 0.7129\n",
      "0\n",
      "Epoch: [59] Avg Error Ratio 0.713; Average Loss 2.627; Avg Time x Batch 0.029\n",
      "Epoch: [60][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (1.2150)\tError Ratio 0.9011 (0.6802) \t\n",
      "Epoch: [60][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.025)\tLoss 1.1189 (2.8130)\tError Ratio 0.6514 (0.7208) \t\n",
      "epoch 61/100, training loss 2.4778, training score 0.7085\n",
      "0\n",
      "Epoch: [60] Avg Error Ratio 0.708; Average Loss 2.478; Avg Time x Batch 0.029\n",
      "Epoch: [61][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (1.1746)\tError Ratio 0.9011 (0.6784) \t\n",
      "Epoch: [61][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (2.6468)\tError Ratio 0.6514 (0.7155) \t\n",
      "epoch 62/100, training loss 2.3415, training score 0.7042\n",
      "0\n",
      "Epoch: [61] Avg Error Ratio 0.704; Average Loss 2.342; Avg Time x Batch 0.029\n",
      "Epoch: [62][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (1.1385)\tError Ratio 0.9011 (0.6767) \t\n",
      "Epoch: [62][40/50]\tTime 0.012 (0.032)\tData 0.006 (0.026)\tLoss 1.1189 (2.4952)\tError Ratio 0.6514 (0.7105) \t\n",
      "epoch 63/100, training loss 2.2172, training score 0.7001\n",
      "0\n",
      "Epoch: [62] Avg Error Ratio 0.700; Average Loss 2.217; Avg Time x Batch 0.029\n",
      "Epoch: [63][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (1.1062)\tError Ratio 0.9011 (0.6750) \t\n",
      "Epoch: [63][40/50]\tTime 0.012 (0.033)\tData 0.006 (0.025)\tLoss 1.1189 (2.3570)\tError Ratio 0.6514 (0.7057) \t\n",
      "epoch 64/100, training loss 2.1039, training score 0.6961\n",
      "0\n",
      "Epoch: [63] Avg Error Ratio 0.696; Average Loss 2.104; Avg Time x Batch 0.029\n",
      "Epoch: [64][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (1.0776)\tError Ratio 0.9011 (0.6734) \t\n",
      "Epoch: [64][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (2.2315)\tError Ratio 0.6514 (0.7010) \t\n",
      "epoch 65/100, training loss 2.0009, training score 0.6923\n",
      "0\n",
      "Epoch: [64] Avg Error Ratio 0.692; Average Loss 2.001; Avg Time x Batch 0.029\n",
      "Epoch: [65][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (1.0523)\tError Ratio 0.9011 (0.6719) \t\n",
      "Epoch: [65][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (2.1176)\tError Ratio 0.6514 (0.6972) \t\n",
      "epoch 66/100, training loss 1.9076, training score 0.6892\n",
      "0\n",
      "Epoch: [65] Avg Error Ratio 0.689; Average Loss 1.908; Avg Time x Batch 0.029\n",
      "Epoch: [66][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (1.0300)\tError Ratio 0.9011 (0.6704) \t\n",
      "Epoch: [66][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.025)\tLoss 1.1189 (2.0133)\tError Ratio 0.6514 (0.6938) \t\n",
      "epoch 67/100, training loss 1.8220, training score 0.6864\n",
      "0\n",
      "Epoch: [66] Avg Error Ratio 0.686; Average Loss 1.822; Avg Time x Batch 0.029\n",
      "Epoch: [67][20/50]\tTime 0.021 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (1.0103)\tError Ratio 0.9011 (0.6691) \t\n",
      "Epoch: [67][40/50]\tTime 0.012 (0.033)\tData 0.006 (0.025)\tLoss 1.1189 (1.9169)\tError Ratio 0.6514 (0.6905) \t\n",
      "epoch 68/100, training loss 1.7430, training score 0.6837\n",
      "0\n",
      "Epoch: [67] Avg Error Ratio 0.684; Average Loss 1.743; Avg Time x Batch 0.029\n",
      "Epoch: [68][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.9929)\tError Ratio 0.9011 (0.6677) \t\n",
      "Epoch: [68][40/50]\tTime 0.011 (0.033)\tData 0.007 (0.026)\tLoss 1.1189 (1.8280)\tError Ratio 0.6514 (0.6881) \t\n",
      "epoch 69/100, training loss 1.6701, training score 0.6817\n",
      "0\n",
      "Epoch: [68] Avg Error Ratio 0.682; Average Loss 1.670; Avg Time x Batch 0.029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [69][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (0.9777)\tError Ratio 0.9011 (0.6665) \t\n",
      "Epoch: [69][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.025)\tLoss 1.1189 (1.7462)\tError Ratio 0.6514 (0.6857) \t\n",
      "epoch 70/100, training loss 1.6030, training score 0.6797\n",
      "0\n",
      "Epoch: [69] Avg Error Ratio 0.680; Average Loss 1.603; Avg Time x Batch 0.029\n",
      "Epoch: [70][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (0.9645)\tError Ratio 0.9011 (0.6653) \t\n",
      "Epoch: [70][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.026)\tLoss 1.1189 (1.6711)\tError Ratio 0.6514 (0.6835) \t\n",
      "epoch 71/100, training loss 1.5414, training score 0.6779\n",
      "0\n",
      "Epoch: [70] Avg Error Ratio 0.678; Average Loss 1.541; Avg Time x Batch 0.029\n",
      "Epoch: [71][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.9531)\tError Ratio 0.9011 (0.6641) \t\n",
      "Epoch: [71][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (1.6022)\tError Ratio 0.6514 (0.6812) \t\n",
      "epoch 72/100, training loss 1.4849, training score 0.6761\n",
      "0\n",
      "Epoch: [71] Avg Error Ratio 0.676; Average Loss 1.485; Avg Time x Batch 0.029\n",
      "Epoch: [72][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.9429)\tError Ratio 0.9011 (0.6630) \t\n",
      "Epoch: [72][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (1.5377)\tError Ratio 0.6514 (0.6788) \t\n",
      "epoch 73/100, training loss 1.4320, training score 0.6740\n",
      "0\n",
      "Epoch: [72] Avg Error Ratio 0.674; Average Loss 1.432; Avg Time x Batch 0.029\n",
      "Epoch: [73][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.9341)\tError Ratio 0.9011 (0.6619) \t\n",
      "Epoch: [73][40/50]\tTime 0.012 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (1.4785)\tError Ratio 0.6514 (0.6764) \t\n",
      "epoch 74/100, training loss 1.3835, training score 0.6721\n",
      "0\n",
      "Epoch: [73] Avg Error Ratio 0.672; Average Loss 1.384; Avg Time x Batch 0.029\n",
      "Epoch: [74][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.9267)\tError Ratio 0.9011 (0.6609) \t\n",
      "Epoch: [74][40/50]\tTime 0.012 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (1.4244)\tError Ratio 0.6514 (0.6740) \t\n",
      "epoch 75/100, training loss 1.3392, training score 0.6702\n",
      "0\n",
      "Epoch: [74] Avg Error Ratio 0.670; Average Loss 1.339; Avg Time x Batch 0.029\n",
      "Epoch: [75][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (0.9205)\tError Ratio 0.9011 (0.6599) \t\n",
      "Epoch: [75][40/50]\tTime 0.012 (0.033)\tData 0.006 (0.025)\tLoss 1.1189 (1.3751)\tError Ratio 0.6514 (0.6718) \t\n",
      "epoch 76/100, training loss 1.2987, training score 0.6683\n",
      "0\n",
      "Epoch: [75] Avg Error Ratio 0.668; Average Loss 1.299; Avg Time x Batch 0.029\n",
      "Epoch: [76][20/50]\tTime 0.021 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (0.9153)\tError Ratio 0.9011 (0.6590) \t\n",
      "Epoch: [76][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.025)\tLoss 1.1189 (1.3302)\tError Ratio 0.6514 (0.6697) \t\n",
      "epoch 77/100, training loss 1.2619, training score 0.6666\n",
      "0\n",
      "Epoch: [76] Avg Error Ratio 0.667; Average Loss 1.262; Avg Time x Batch 0.029\n",
      "Epoch: [77][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (0.9111)\tError Ratio 0.9011 (0.6581) \t\n",
      "Epoch: [77][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.025)\tLoss 1.1189 (1.2893)\tError Ratio 0.6514 (0.6678) \t\n",
      "epoch 78/100, training loss 1.2284, training score 0.6651\n",
      "0\n",
      "Epoch: [77] Avg Error Ratio 0.665; Average Loss 1.228; Avg Time x Batch 0.029\n",
      "Epoch: [78][20/50]\tTime 0.020 (0.048)\tData 0.015 (0.039)\tLoss 1.6279 (0.9077)\tError Ratio 0.9011 (0.6573) \t\n",
      "Epoch: [78][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.026)\tLoss 1.1189 (1.2522)\tError Ratio 0.6514 (0.6663) \t\n",
      "epoch 79/100, training loss 1.1980, training score 0.6639\n",
      "0\n",
      "Epoch: [78] Avg Error Ratio 0.664; Average Loss 1.198; Avg Time x Batch 0.029\n",
      "Epoch: [79][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.9051)\tError Ratio 0.9011 (0.6566) \t\n",
      "Epoch: [79][40/50]\tTime 0.012 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (1.2187)\tError Ratio 0.6514 (0.6649) \t\n",
      "epoch 80/100, training loss 1.1704, training score 0.6627\n",
      "0\n",
      "Epoch: [79] Avg Error Ratio 0.663; Average Loss 1.170; Avg Time x Batch 0.029\n",
      "Epoch: [80][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.9030)\tError Ratio 0.9011 (0.6559) \t\n",
      "Epoch: [80][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (1.1883)\tError Ratio 0.6514 (0.6635) \t\n",
      "epoch 81/100, training loss 1.1455, training score 0.6615\n",
      "0\n",
      "Epoch: [80] Avg Error Ratio 0.662; Average Loss 1.146; Avg Time x Batch 0.029\n",
      "Epoch: [81][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.9015)\tError Ratio 0.9011 (0.6552) \t\n",
      "Epoch: [81][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (1.1609)\tError Ratio 0.6514 (0.6622) \t\n",
      "epoch 82/100, training loss 1.1230, training score 0.6605\n",
      "0\n",
      "Epoch: [81] Avg Error Ratio 0.660; Average Loss 1.123; Avg Time x Batch 0.029\n",
      "Epoch: [82][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (0.9004)\tError Ratio 0.9011 (0.6546) \t\n",
      "Epoch: [82][40/50]\tTime 0.012 (0.033)\tData 0.006 (0.025)\tLoss 1.1189 (1.1361)\tError Ratio 0.6514 (0.6609) \t\n",
      "epoch 83/100, training loss 1.1027, training score 0.6594\n",
      "0\n",
      "Epoch: [82] Avg Error Ratio 0.659; Average Loss 1.103; Avg Time x Batch 0.029\n",
      "Epoch: [83][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (0.8997)\tError Ratio 0.9011 (0.6540) \t\n",
      "Epoch: [83][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.025)\tLoss 1.1189 (1.1137)\tError Ratio 0.6514 (0.6597) \t\n",
      "epoch 84/100, training loss 1.0844, training score 0.6584\n",
      "0\n",
      "Epoch: [83] Avg Error Ratio 0.658; Average Loss 1.084; Avg Time x Batch 0.029\n",
      "Epoch: [84][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.8993)\tError Ratio 0.9011 (0.6535) \t\n",
      "Epoch: [84][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.026)\tLoss 1.1189 (1.0935)\tError Ratio 0.6514 (0.6586) \t\n",
      "epoch 85/100, training loss 1.0678, training score 0.6575\n",
      "0\n",
      "Epoch: [84] Avg Error Ratio 0.657; Average Loss 1.068; Avg Time x Batch 0.029\n",
      "Epoch: [85][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.8991)\tError Ratio 0.9011 (0.6530) \t\n",
      "Epoch: [85][40/50]\tTime 0.011 (0.032)\tData 0.006 (0.026)\tLoss 1.1189 (1.0753)\tError Ratio 0.6514 (0.6575) \t\n",
      "epoch 86/100, training loss 1.0529, training score 0.6566\n",
      "0\n",
      "Epoch: [85] Avg Error Ratio 0.657; Average Loss 1.053; Avg Time x Batch 0.029\n",
      "Epoch: [86][20/50]\tTime 0.020 (0.048)\tData 0.015 (0.040)\tLoss 1.6279 (0.8991)\tError Ratio 0.9011 (0.6530) \t\n",
      "Epoch: [86][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.026)\tLoss 1.1189 (1.0588)\tError Ratio 0.6514 (0.6566) \t\n",
      "epoch 87/100, training loss 1.0394, training score 0.6558\n",
      "0\n",
      "Epoch: [86] Avg Error Ratio 0.656; Average Loss 1.039; Avg Time x Batch 0.029\n",
      "Epoch: [87][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (0.8992)\tError Ratio 0.9011 (0.6533) \t\n",
      "Epoch: [87][40/50]\tTime 0.012 (0.033)\tData 0.006 (0.025)\tLoss 1.1189 (1.0440)\tError Ratio 0.6514 (0.6560) \t\n",
      "epoch 88/100, training loss 1.0272, training score 0.6553\n",
      "0\n",
      "Epoch: [87] Avg Error Ratio 0.655; Average Loss 1.027; Avg Time x Batch 0.029\n",
      "Epoch: [88][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.8994)\tError Ratio 0.9011 (0.6537) \t\n",
      "Epoch: [88][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (1.0305)\tError Ratio 0.6514 (0.6553) \t\n",
      "epoch 89/100, training loss 1.0162, training score 0.6548\n",
      "0\n",
      "Epoch: [88] Avg Error Ratio 0.655; Average Loss 1.016; Avg Time x Batch 0.029\n",
      "Epoch: [89][20/50]\tTime 0.021 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.8997)\tError Ratio 0.9011 (0.6540) \t\n",
      "Epoch: [89][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (1.0184)\tError Ratio 0.6514 (0.6547) \t\n",
      "epoch 90/100, training loss 1.0062, training score 0.6543\n",
      "0\n",
      "Epoch: [89] Avg Error Ratio 0.654; Average Loss 1.006; Avg Time x Batch 0.029\n",
      "Epoch: [90][20/50]\tTime 0.020 (0.048)\tData 0.015 (0.039)\tLoss 1.6279 (0.9000)\tError Ratio 0.9011 (0.6543) \t\n",
      "Epoch: [90][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (1.0074)\tError Ratio 0.6514 (0.6542) \t\n",
      "epoch 91/100, training loss 0.9972, training score 0.6539\n",
      "0\n",
      "Epoch: [90] Avg Error Ratio 0.654; Average Loss 0.997; Avg Time x Batch 0.029\n",
      "Epoch: [91][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.9003)\tError Ratio 0.9011 (0.6545) \t\n",
      "Epoch: [91][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (0.9975)\tError Ratio 0.6514 (0.6536) \t\n",
      "epoch 92/100, training loss 0.9891, training score 0.6534\n",
      "0\n",
      "Epoch: [91] Avg Error Ratio 0.653; Average Loss 0.989; Avg Time x Batch 0.029\n",
      "Epoch: [92][20/50]\tTime 0.020 (0.048)\tData 0.015 (0.039)\tLoss 1.6279 (0.9006)\tError Ratio 0.9011 (0.6547) \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [92][40/50]\tTime 0.011 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (0.9885)\tError Ratio 0.6514 (0.6530) \t\n",
      "epoch 93/100, training loss 0.9817, training score 0.6529\n",
      "0\n",
      "Epoch: [92] Avg Error Ratio 0.653; Average Loss 0.982; Avg Time x Batch 0.029\n",
      "Epoch: [93][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.9009)\tError Ratio 0.9011 (0.6549) \t\n",
      "Epoch: [93][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.026)\tLoss 1.1189 (0.9802)\tError Ratio 0.6514 (0.6524) \t\n",
      "epoch 94/100, training loss 0.9749, training score 0.6524\n",
      "0\n",
      "Epoch: [93] Avg Error Ratio 0.652; Average Loss 0.975; Avg Time x Batch 0.029\n",
      "Epoch: [94][20/50]\tTime 0.021 (0.048)\tData 0.015 (0.039)\tLoss 1.6279 (0.9012)\tError Ratio 0.9011 (0.6550) \t\n",
      "Epoch: [94][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.026)\tLoss 1.1189 (0.9727)\tError Ratio 0.6514 (0.6519) \t\n",
      "epoch 95/100, training loss 0.9688, training score 0.6520\n",
      "0\n",
      "Epoch: [94] Avg Error Ratio 0.652; Average Loss 0.969; Avg Time x Batch 0.029\n",
      "Epoch: [95][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.9014)\tError Ratio 0.9011 (0.6551) \t\n",
      "Epoch: [95][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.026)\tLoss 1.1189 (0.9659)\tError Ratio 0.6514 (0.6513) \t\n",
      "epoch 96/100, training loss 0.9631, training score 0.6515\n",
      "0\n",
      "Epoch: [95] Avg Error Ratio 0.652; Average Loss 0.963; Avg Time x Batch 0.029\n",
      "Epoch: [96][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.9015)\tError Ratio 0.9011 (0.6552) \t\n",
      "Epoch: [96][40/50]\tTime 0.012 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (0.9595)\tError Ratio 0.6514 (0.6508) \t\n",
      "epoch 97/100, training loss 0.9579, training score 0.6511\n",
      "0\n",
      "Epoch: [96] Avg Error Ratio 0.651; Average Loss 0.958; Avg Time x Batch 0.029\n",
      "Epoch: [97][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.039)\tLoss 1.6279 (0.9017)\tError Ratio 0.9011 (0.6553) \t\n",
      "Epoch: [97][40/50]\tTime 0.012 (0.033)\tData 0.007 (0.026)\tLoss 1.1189 (0.9537)\tError Ratio 0.6514 (0.6502) \t\n",
      "epoch 98/100, training loss 0.9532, training score 0.6506\n",
      "0\n",
      "Epoch: [97] Avg Error Ratio 0.651; Average Loss 0.953; Avg Time x Batch 0.029\n",
      "Epoch: [98][20/50]\tTime 0.020 (0.048)\tData 0.015 (0.039)\tLoss 1.6279 (0.9017)\tError Ratio 0.9011 (0.6553) \t\n",
      "Epoch: [98][40/50]\tTime 0.012 (0.033)\tData 0.006 (0.026)\tLoss 1.1189 (0.9484)\tError Ratio 0.6514 (0.6497) \t\n",
      "epoch 99/100, training loss 0.9488, training score 0.6502\n",
      "0\n",
      "Epoch: [98] Avg Error Ratio 0.650; Average Loss 0.949; Avg Time x Batch 0.029\n",
      "Epoch: [99][20/50]\tTime 0.020 (0.047)\tData 0.015 (0.038)\tLoss 1.6279 (0.9017)\tError Ratio 0.9011 (0.6553) \t\n",
      "Epoch: [99][40/50]\tTime 0.012 (0.033)\tData 0.006 (0.025)\tLoss 1.1189 (0.9434)\tError Ratio 0.6514 (0.6492) \t\n",
      "epoch 100/100, training loss 0.9447, training score 0.6498\n",
      "0\n",
      "Epoch: [99] Avg Error Ratio 0.650; Average Loss 0.945; Avg Time x Batch 0.029\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_error = [], [] \n",
    "for epoch in range(100):\n",
    "    loss, error = train(train_loader= train_loader, \n",
    "                        model=model,\n",
    "                        criterion=criterion, \n",
    "                        evaluation=evaluation, \n",
    "                        epoch=epoch, \n",
    "                        optimizer=optimizer)\n",
    "    train_loss.append(loss), train_error.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.array(train_loss)\n",
    "train_error = np.array(train_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b6a7b1a6090>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(100),train_loss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b6a81976e50>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(100),train_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 14 15:13:02 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |\r\n",
      "| N/A   40C    P0    37W / 250W |   1222MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000000:D8:00.0 Off |                    0 |\r\n",
      "| N/A   36C    P0    27W / 250W |     11MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0    279066      C   ...rumalla.s/anaconda3/envs/dgl/bin/python  1211MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
